{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rileva e analizza i volti\n",
    "\n",
    "Le soluzioni di visione artificiale spesso richiedono una soluzione di intelligenza artificiale (IA) per essere in grado di rilevare, analizzare o identificare i volti umani. Ad esempio, supponiamo che l'azienda di vendita al dettaglio Northwind Traders abbia deciso di implementare un \"negozio intelligente\", in cui i servizi di IA monitorano il negozio per identificare i clienti che richiedono assistenza e guidano i dipendenti nell'aiutarli. Un modo per farlo è quello di eseguire il rilevamento e l'analisi dei volti: in altre parole, determinare se ci sono dei volti nelle immagini, e in tal caso analizzarne le caratteristiche.\n",
    "\n",
    "![Un robot che analizza un volto](./images/face_analysis.jpg)\n",
    "\n",
    "## Utilizza il servizio cognitivo Viso per rilevare i visi\n",
    "\n",
    "Supponiamo che il sistema di negozio intelligente che Northwind Traders vuole creare debba essere in grado di rilevare i clienti e analizzarne le caratteristiche del volto. In Microsoft Azure, è possibile farlo utilizzando **Volto**, parte dei servizi cognitivi di Azure.\n",
    "\n",
    "### Crea una risorsa di Servizi cognitivi\n",
    "\n",
    "Iniziamo creando una risorsa di **Servizi cognitivi** nella tua sottoscrizione di Azure.\n",
    "\n",
    "> **Nota**: Se disponi già di una risorsa di Servizi Cognitivi, basta aprire la sua pagina **Avvio rapido** nel portale di Azure e copiare la sua chiave e l'endpoint nella cella seguente. Altrimenti, procedi come segue per crearne una.\n",
    "\n",
    "1. In un'altra scheda del browser, apri il portale di Azure all'indirizzo https://portal.azure.com, accedendo con il tuo account Microsoft.\n",
    "2. Fai clic sul pulsante **&#65291;Crea una risorsa**, cerca *Servizi cognitivi* e crea una risorsa di **Servizi cognitivi** con le impostazioni seguenti:\n",
    "    - **Sottoscrizione**: *La tua sottoscrizione di Azure*.\n",
    "    - **Gruppo di risorse**: *Seleziona o crea un gruppo di risorse con un nome univoco*.\n",
    "    - **Area geografica**: *Scegli una qualsiasi area disponibile*:\n",
    "    - **Nome**: *Immetti un nome univoco*.\n",
    "    - **Piano tariffario**: S0\n",
    "    - **Confermo di aver letto e compreso gli avvisi**: Selezionato.\n",
    "3. Attendi il completamento della distribuzione. Vai quindi alla tua risorsa di servizi cognitivi e, nella pagina **Panoramica**, fai clic sul link per gestire le chiavi per il servizio. Avrai bisogno dell'endpoint e delle chiavi per connetterti alla tua risorsa di servizi cognitivi dalle applicazioni client.\n",
    "\n",
    "### Ottieni la chiave e l'endpoint per la tua risorsa di Servizi cognitivi\n",
    "\n",
    "Per usare la risorsa di servizi cognitivi, le applicazioni client hanno bisogno del loro endpoint e della chiave di autenticazione:\n",
    "\n",
    "1. Nel portale di Azure, nella pagina **Chiavi ed endpoint** per la tua risorsa di servizio cognitivo, copia la **Key1** per la tua risorsa e incollala nel codice sottostante, sostituendo **YOUR_COG_KEY**.\n",
    "\n",
    "2. Copia l'**endpoint** per la tua risorsa e incollalo nel codice sottostante, sostituendo **YOUR_COG_ENDPOINT**.\n",
    "\n",
    "3. Esegui il codice nella cella di seguito facendo clic sul pulsante Esegui cella <span>&#9655;</span> (in alto a sinistra della cella)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693964655
    }
   },
   "outputs": [],
   "source": [
    "cog_key = 'YOUR_COG_KEY'\n",
    "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
    "\n",
    "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora che disponi di una risorsa di Servizi Cognitivi puoi usare il servizio Viso per rilevare i visi umani nel negozio.\n",
    "\n",
    "Esegui la cella di codice qui sotto per vedere un esempio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693970079
    }
   },
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from python_code import faces\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a face detection client.\n",
    "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
    "\n",
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "\n",
    "# Display the faces (code in python_code/faces.py)\n",
    "faces.show_faces(image_path, detected_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ciascun volto rilevato viene assegnato un ID univoco, in modo che l'applicazione possa identificare ogni singolo volto che è stato rilevato.\n",
    "\n",
    "Esegui la cella seguente per vedere gli ID di altri volti di acquirenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693970447
    }
   },
   "outputs": [],
   "source": [
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "\n",
    "# Display the faces (code in python_code/faces.py)\n",
    "faces.show_faces(image_path, detected_faces, show_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizza gli attributi del volto\n",
    "\n",
    "Viso non soltanto rileva i volti, ma è anche in grado di analizzarne le caratteristiche e analizzare le espressioni facciali per suggerire l'età e lo stato emotivo. Ad esempio, esegui il codice seguente per analizzare gli attributi del volto di un acquirente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693971321
    }
   },
   "outputs": [],
   "source": [
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam1.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces and specified facial attributes\n",
    "attributes = ['age', 'emotion']\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream, return_face_attributes=attributes)\n",
    "\n",
    "# Display the faces and attributes (code in python_code/faces.py)\n",
    "faces.show_face_attributes(image_path, detected_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sulla base dei punteggi emozionali rilevati per il cliente nell'immagine, il cliente sembra piuttosto soddisfatto dell'esperienza di acquisto.\n",
    "\n",
    "## Trova volti simili \n",
    "\n",
    "Gli ID dei volti che vengono creati per ogni volto rilevato vengono utilizzati per identificare individualmente i volti rilevati. Puoi usare questi ID per confrontare un volto rilevato con quelli rilevati in precedenza e trovare volti con caratteristiche simili.\n",
    "\n",
    "Ad esempio, esegui la cella seguente per confrontare l'acquirente di un'immagine con gli acquirenti di un'altra, e trova un volto corrispondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693972555
    }
   },
   "outputs": [],
   "source": [
    "# Get the ID of the first face in image 1\n",
    "image_1_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
    "image_1_stream = open(image_1_path, \"rb\")\n",
    "image_1_faces = face_client.face.detect_with_stream(image=image_1_stream)\n",
    "face_1 = image_1_faces[0]\n",
    "\n",
    "# Get the face IDs in a second image\n",
    "image_2_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
    "image_2_stream = open(image_2_path, \"rb\")\n",
    "image_2_faces = face_client.face.detect_with_stream(image=image_2_stream)\n",
    "image_2_face_ids = list(map(lambda face: face.face_id, image_2_faces))\n",
    "\n",
    "# Find faces in image 2 that are similar to the one in image 1\n",
    "similar_faces = face_client.face.find_similar(face_id=face_1.face_id, face_ids=image_2_face_ids)\n",
    "\n",
    "# Show the face in image 1, and similar faces in image 2(code in python_code/face.py)\n",
    "faces.show_similar_faces(image_1_path, face_1, image_2_path, image_2_faces, similar_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riconosci i volti\n",
    "\n",
    "Finora hai visto come Viso sia in grado di rilevare i visi e le caratteristiche facciali e di identificare due volti che sono simili tra loro. Puoi compiere un ulteriore passo in avanti integrando una soluzione di *riconoscimento facciale* in cui esegui il training di Viso per riconoscere il viso di una specifica persona. Questa operazione può essere utile in una varietà di scenari, ad esempio per aggiungere automaticamente tag alle fotografie degli amici in un'applicazione di social media, o per utilizzare il riconoscimento facciale come parte di un sistema di verifica dell'identità biometrica.\n",
    "\n",
    "Per vedere come funziona, supponiamo che l'azienda Northwind Traders voglia usare il riconoscimento facciale per assicurarsi che solo i dipendenti autorizzati del reparto IT possano accedere a sistemi sicuri.\n",
    "\n",
    "Inizieremo creando un *gruppo di persone* che rappresenta i dipendenti autorizzati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693973492
    }
   },
   "outputs": [],
   "source": [
    "group_id = 'employee_group_id'\n",
    "try:\n",
    "    # Delete group if it already exists\n",
    "    face_client.person_group.delete(group_id)\n",
    "except Exception as ex:\n",
    "    print(ex.message)\n",
    "finally:\n",
    "    face_client.person_group.create(group_id, 'employees')\n",
    "    print ('Group created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora che il *gruppo di persone* esiste, possiamo aggiungere una *persona* per ogni dipendente che vogliamo includere nel gruppo e registrare quindi più fotografie di ogni persona in modo che Viso possa apprendere le distinte caratteristiche facciali di ogni persona. Idealmente, le immagini dovrebbero mostrare la stessa persona in pose diverse e con diverse espressioni facciali.\n",
    "\n",
    "Aggiungeremo un singolo dipendente chiamato Giovanni e registreremo tre fotografie del dipendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693976898
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Add a person (Wendell) to the group\n",
    "wendell = face_client.person_group_person.create(group_id, 'Wendell')\n",
    "\n",
    "# Get photo's of Wendell\n",
    "folder = os.path.join('data', 'face', 'wendell')\n",
    "wendell_pics = os.listdir(folder)\n",
    "\n",
    "# Register the photos\n",
    "i = 0\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for pic in wendell_pics:\n",
    "    # Add each photo to person in person group\n",
    "    img_path = os.path.join(folder, pic)\n",
    "    img_stream = open(img_path, \"rb\")\n",
    "    face_client.person_group_person.add_face_from_stream(group_id, wendell.person_id, img_stream)\n",
    "\n",
    "    # Display each image\n",
    "    img = Image.open(img_path)\n",
    "    i +=1\n",
    "    a=fig.add_subplot(1,len(wendell_pics), i)\n",
    "    a.axis('off')\n",
    "    imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la persona aggiunta e le fotografie registrate, possiamo ora eseguire il training di Viso affinché riconosca ogni persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693977046
    }
   },
   "outputs": [],
   "source": [
    "face_client.person_group.train(group_id)\n",
    "print('Trained!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora, con il modello sottoposto a training, puoi usarlo per identificare i volti riconosciuti in un'immagine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693994820
    }
   },
   "outputs": [],
   "source": [
    "# Get the face IDs in a second image\n",
    "image_path = os.path.join('data', 'face', 'employees.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "image_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "image_face_ids = list(map(lambda face: face.face_id, image_faces))\n",
    "\n",
    "# Get recognized face names\n",
    "face_names = {}\n",
    "recognized_faces = face_client.face.identify(image_face_ids, group_id)\n",
    "for face in recognized_faces:\n",
    "    person_name = face_client.person_group_person.get(group_id, face.candidates[0].person_id).name\n",
    "    face_names[face.face_id] = person_name\n",
    "\n",
    "# show recognized faces\n",
    "faces.show_recognized_faces(image_path, image_faces, face_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scopri di più\n",
    "\n",
    "Per saperne di più sul servizio cognitivo Viso, consulta la [documentazione di Viso](https://docs.microsoft.com/azure/cognitive-services/face/)\r\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}