{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Voce\r\n",
        "\r\n",
        "Sempre più spesso, ci aspettiamo di poter comunicare con i sistemi di intelligenza artificiale (IA) parlando loro, spesso con l'aspettativa di una risposta parlata.\r\n",
        "\r\n",
        "![Un robot che parla](./images/speech.jpg)\r\n",
        "\r\n",
        "Il *riconoscimento vocale* (un sistema di AI che interpreta il linguaggio parlato) e la *sintesi vocale* (un sistema di AI che genera una risposta parlata) sono i componenti chiave di una soluzione di AI con abilitazione vocale.\r\n",
        "\r\n",
        "## Crea una risorsa di servizi cognitivi\r\n",
        "\r\n",
        "Per sviluppare un software in grado di interpretare una voce udibile e rispondere verbalmente, è possibile utilizzare il servizio cognitivo **Voce**, che fornisce un modo semplice per trascrivere il linguaggio parlato in testo e viceversa.\r\n",
        "\r\n",
        "Se non ne hai già una, procedi come segue per creare una risorsa di **Servizi Cognitivi** nella tua sottoscrizione di Azure:\r\n",
        "\r\n",
        "> **Nota**: Se disponi già di una risorsa di Servizi Cognitivi, basta aprire la sua pagina **Avvio rapido** nel portale di Azure e copiare la sua chiave e l'endpoint nella cella seguente. Altrimenti, procedi come segue per crearne una.\r\n",
        "\r\n",
        "1. In un'altra scheda del browser, apri il portale di Azure all'indirizzo https://portal.azure.com, accedendo con il tuo account Microsoft.\r\n",
        "2. Fai clic sul pulsante **&#65291;Crea una risorsa**, cerca *Servizi cognitivi* e crea una risorsa di **Servizi cognitivi** con le impostazioni seguenti:\r\n",
        "    - **Sottoscrizione**: *La tua sottoscrizione di Azure*.\r\n",
        "    - **Gruppo di risorse**: *Seleziona o crea un gruppo di risorse con un nome univoco*.\r\n",
        "    - **Area geografica**: *Scegli una qualsiasi area disponibile*:\r\n",
        "    - **Nome**: *Immetti un nome univoco*.\r\n",
        "    - **Piano tariffario**: S0\r\n",
        "    - **Confermo di aver letto e compreso gli avvisi**: Selezionato.\r\n",
        "3. Attendi il completamento della distribuzione. Vai quindi alla tua risorsa di servizi cognitivi e, nella pagina **Panoramica**, fai clic sul link per gestire le chiavi per il servizio. Avrai bisogno della chiave e della posizione per connetterti alla tua risorsa di servizi cognitivi dalle applicazioni client.\r\n",
        "\r\n",
        "### Ottieni la chiave e la posizione per la tua risorsa di Servizi cognitivi\r\n",
        "\r\n",
        "Per usare la risorsa di servizi cognitivi, le applicazioni client hanno bisogno della chiave di autenticazione e della posizione:\r\n",
        "\r\n",
        "1. Nel portale di Azure, nella pagina **Chiavi ed endpoint** per la tua risorsa di servizio cognitivo, copia la **Key1** per la tua risorsa e incollala nel codice sottostante, sostituendo **YOUR_COG_KEY**.\r\n",
        "2. Copia la **Posizione** per la tua risorsa e incollala nel codice sottostante, sostituendo **YOUR_COG_LOCATION**.\r\n",
        ">**Nota**: Rimani nella pagina **Chiavi ed endpoint** e copia la **Posizione** da questa pagina (esempio: _westus_). Non aggiungere spazi tra le parole per il campo Posizione. \r\n",
        "3. Esegui il codice seguente facendo clic sul pulsante **Esegui cella** (&#9655;) a sinistra della cella."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599695240794
        }
      },
      "outputs": [],
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_location = 'YOUR_COG_LOCATION'\n",
        "\n",
        "print('Ready to use cognitive services in {} using key {}'.format(cog_location, cog_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Riconoscimento vocale\r\n",
        "\r\n",
        "Supponiamo che tu voglia creare un sistema di automazione domestica che accetti istruzioni vocali, come \"accendi la luce\" o \"spegni la luce\". La tua applicazione deve essere in grado di prendere l'input basato sull'audio (la tua istruzione parlata), e interpretarlo trascrivendolo in testo che può poi analizzare.\r\n",
        "\r\n",
        "È tutto pronto per trascrivere una voce. L'input può venire da un **microfono**o da un **file audio**. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Riconoscimento vocale con un file audio\r\n",
        "\r\n",
        "Esegui la cella seguente per vedere il servizio di riconoscimento vocale in azione con un **file audio**. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from playsound import playsound\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
        "\n",
        "# Get spoken command from audio file\r\n",
        "file_name = 'light-on.wav'\n",
        "audio_file = os.path.join('data', 'speech', file_name)\n",
        "\n",
        "# Configure speech recognizer\r\n",
        "speech_config = SpeechConfig(cog_key, cog_location)\n",
        "speech_config.speech_synthesis_voice_name = 'en-US-ChristopherNeural'\n",
        "audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n",
        "speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n",
        "\n",
        "# Use a one-time, synchronous call to transcribe the speech\r\n",
        "speech = speech_recognizer.recognize_once()\n",
        "\n",
        "# Play the original audio file\r\n",
        "playsound(audio_file)\n",
        "\n",
        "# Show transcribed text from audio file\r\n",
        "print(speech.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sintesi vocale\r\n",
        "\r\n",
        "Dunque ora hai visto in che modo è possibile usare il servizio Voce per trascrivere il parlato in testo; e per quanto riguarda il contrario? Come si converte il testo in parlato?\r\n",
        "\r\n",
        "Supponiamo che il tuo sistema domotico abbia interpretato un comando per accendere la luce. Una risposta appropriata potrebbe essere quella di confermare verbalmente il comando (oltre a eseguire effettivamente il compito richiesto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599695261170
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
        "%matplotlib inline\n",
        "\n",
        "# Get text to be spoken\r\n",
        "response_text = 'Turning the light on.'\n",
        "\n",
        "# Configure speech synthesis\r\n",
        "speech_config = SpeechConfig(cog_key, cog_location)\n",
        "speech_config.speech_synthesis_voice_name = 'en-US-ChristopherNeural'\n",
        "speech_synthesizer = SpeechSynthesizer(speech_config)\n",
        "\n",
        "# Transcribe text into speech\r\n",
        "result = speech_synthesizer.speak_text(response_text)\n",
        "\n",
        "# Display an appropriate image \r\n",
        "file_name = response_text.lower() + \"jpg\"\n",
        "img = Image.open(os.path.join(\"data\", \"speech\", file_name))\n",
        "plt.axis('off')\n",
        "plt. imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prova a cambiare la variabile **response_text** in *Spegnere la luce.* (compreso il punto finale) ed esegui nuovamente la cella per sentire il risultato.\r\n",
        "\r\n",
        "## Scopri di più\r\n",
        "\r\n",
        "Hai visto un esempio molto semplice dell'utilizzo del servizio cognitivo Voce in questo notebook. Puoi saperne di più su [conversione della voce in testo scritto](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-speech-to-text) e [sintesi vocale](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-text-to-speech) nella documentazione del servizio Voce."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 32-bit",
      "metadata": {
        "interpreter": {
          "hash": "177429bd1865e7f7a0dbecbac90518c0d9641b1102b2e6c0df4b82dc948b5cb2"
        }
      },
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}