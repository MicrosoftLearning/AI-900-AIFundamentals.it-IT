{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Riconoscimento ottico dei caratteri\r\n",
        "\r\n",
        "![Un robot che legge un giornale](./images/ocr.jpg)\r\n",
        "\r\n",
        "Una sfida comune della visione artificiale è quella di rilevare e interpretare il testo in un'immagine. Questo tipo di elaborazione è spesso indicato come *riconoscimento ottico dei caratteri* (OCR).\r\n",
        "\r\n",
        "## Usa il servizio Visione artificiale per leggere il testo in un'immagine\r\n",
        "\r\n",
        "Il servizio cognitivo **Visione artificiale** fornisce supporto per attività OCR, tra cui:\r\n",
        "\r\n",
        "- Un'API **OCR** utilizzabile per leggere il testo in più lingue. Quest'API può essere utilizzata in modo sincrono e funziona bene quando è necessario rilevare e leggere una piccola quantità di testo in un'immagine.\r\n",
        "- Un'API **Read** che è ottimizzata per documenti di dimensioni maggiori. Quest'API è usata in modo asincrono e può essere usata sia per il testo stampato che per quello scritto a mano.\r\n",
        "\r\n",
        "Puoi utilizzare questo servizio creando una risorsa di **Visione artificiale** o una risorsa di **Servizi cognitivi**.\r\n",
        "\r\n",
        "Se non l'hai ancora fatto, crea una risorsa di **Servizi Cognitivi** nella tua sottoscrizione di Azure.\r\n",
        "\r\n",
        "> **Nota**: Se disponi già di una risorsa di Servizi Cognitivi, basta aprire la sua pagina **Avvio rapido** nel portale di Azure e copiare la sua chiave e l'endpoint nella cella seguente. Altrimenti, procedi come segue per crearne una.\r\n",
        "\r\n",
        "1. In un'altra scheda del browser, apri il portale Azure all'indirizzo https://portal.azure.com e accedi con il tuo account Microsoft.\r\n",
        "\r\n",
        "2. Fai clic sul pulsante **&#65291;Crea una risorsa**, cerca *Servizi cognitivi* e crea una risorsa di **Servizi cognitivi** con le impostazioni seguenti:\r\n",
        "    - **Sottoscrizione**: *La tua sottoscrizione di Azure*.\r\n",
        "    - **Gruppo di risorse**: *Seleziona o crea un gruppo di risorse con un nome univoco*.\r\n",
        "    - **Area geografica**: *Scegli una qualsiasi area disponibile*:\r\n",
        "    - **Nome**: *Immetti un nome univoco*.\r\n",
        "    - **Piano tariffario**: S0\r\n",
        "    - **Confermo di aver letto e compreso gli avvisi**: Selezionato.\r\n",
        "3. Attendi il completamento della distribuzione. Vai quindi alla tua risorsa di servizi cognitivi e, nella pagina **Panoramica**, fai clic sul link per gestire le chiavi per il servizio. Avrai bisogno dell'endpoint e delle chiavi per connetterti alla tua risorsa di servizi cognitivi dalle applicazioni client.\r\n",
        "\r\n",
        "### Ottieni la chiave e l'endpoint per la tua risorsa di Servizi cognitivi\r\n",
        "\r\n",
        "Per usare la risorsa di servizi cognitivi, le applicazioni client hanno bisogno del loro endpoint e della chiave di autenticazione:\r\n",
        "\r\n",
        "1. Nel portale di Azure, nella pagina **Chiavi ed endpoint** per la tua risorsa di servizio cognitivo, copia la **Key1** per la tua risorsa e incollala nel codice sottostante, sostituendo **YOUR_COG_KEY**.\r\n",
        "2. Copia l'**endpoint** per la tua risorsa e incollalo nel codice sottostante, sostituendo **YOUR_COG_ENDPOINT**.\r\n",
        "3. Esegui il codice nella cella qui sotto facendo clic sul pulsante **Esegui cella** (&#9655;) a sinistra della cella."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694246277
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora che hai impostato la chiave e l'endpoint, puoi usare la tua risorsa del servizio Visione artificiale per estrarre testo da un'immagine.\r\n",
        "\r\n",
        "Iniziamo con l'API **OCR**, che permette di analizzare sincronicamente un'immagine e leggere qualsiasi testo in essa contenuto. In questo caso, hai un'immagine pubblicitaria della società fittizia di vendita al dettaglio Northwind Traders, in cui è presente del testo. Esegui la cella seguente per leggerlo. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'advert.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Use the Computer Vision service to find text in the image\r\n",
        "read_results = computervision_client.recognize_printed_text_in_stream(image_stream)\n",
        "\n",
        "# Process the text line by line\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Read the words in the line of text\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Open image to display it.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694257280
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il testo trovato nell'immagine è organizzato in una struttura gerarchica di aree geografiche, linee e parole, che il codice legge per recuperare i risultati.\r\n",
        "\r\n",
        "Nei risultati, visualizza il testo che è stato letto sopra l'immagine. \r\n",
        "\r\n",
        "## Visualizza i riquadri delimitatori del testo\r\n",
        "\r\n",
        "I risultati includono anche le coordinate dei *riquadri delimitatori del testo* per le righe di testo e le singole parole trovate nell'immagine. Esegui la cella seguente per vedere i riquadri delimitatori del testo per le linee di testo nell'immagine pubblicitaria che hai recuperato sopra."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Open image to display it.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "\n",
        "# Process the text line by line\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Show the position of the line of text\n",
        "        l,t,w,h = list(map(int, line.bounding_box.split(',')))\n",
        "        draw.rectangle(((l,t), (l+w, t+h)), outline='magenta', width=5)\n",
        "\n",
        "        # Read the words in the line of text\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Show the image with the text locations highlighted\r\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694266106
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nel risultato, il riquadro delimitatore del testo per ogni riga di testo è mostrato come un rettangolo sull'immagine.\r\n",
        "\r\n",
        "## Utilizza l'API Read\r\n",
        "\r\n",
        "L'API OCR che hai usato in precedenza funziona bene per immagini con una piccola quantità di testo. Quando hai bisogno di leggere corpi di testo maggiori, come i documenti scansionati, puoi usare l'API **Read**. Per farlo è necessario un processo in più passaggi:\r\n",
        "\r\n",
        "1. Invia un'immagine al servizio Visione artificiale per leggerla e analizzarla in modo asincrono.\r\n",
        "2. Attendi il completamento dell'operazione di analisi.\r\n",
        "3. Recupera i risultati dell'analisi.\r\n",
        "\r\n",
        "Esegui la cella seguente per utilizzare questo processo e leggere il testo di una lettera scansionata al responsabile di un negozio Northwind Traders."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'letter.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Submit a request to read printed text in the image and get the operation ID\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Wait for the asynchronous operation to complete\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# If the operation was successfuly, process the text line by line\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Open image and display it.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694312346
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esamina i risultati. È presente una trascrizione completa della lettera, che consiste per lo più in testo stampato con una firma scritta a mano. L'immagine originale della lettera è mostrata sotto i risultati OCR (potrebbe essere necessario scorrere per vederla).\r\n",
        "\r\n",
        "## Leggi il testo scritto a mano\r\n",
        "\r\n",
        "Nell'esempio precedente, la richiesta di analizzare l'immagine specificava una modalità di riconoscimento del testo che ottimizzava l'operazione per il testo *stampato*. Tieni presente che nonostante ciò, la firma scritta a mano è stata letta.\r\n",
        "\r\n",
        "Questa capacità di leggere il testo scritto a mano è estremamente utile. Ad esempio, supponiamo che tu abbia scritto una nota contenente una lista della spesa, e vuoi usare un'applicazione sul tuo telefono per leggere la nota e trascrivere il testo che contiene.\r\n",
        "\r\n",
        "Esegui la cella seguente per vedere un esempio di operazione di lettura di una lista della spesa scritta a mano."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'note.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Submit a request to read printed text in the image and get the operation ID\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Wait for the asynchronous operation to complete\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# If the operation was successfuly, process the text line by line\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Open image and display it.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694340593
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ulteriori informazioni\r\n",
        "\r\n",
        "Per ulteriori informazioni sull'uso del servizio Visione artificiale per l'OCR, consulta la [documentazione di Visione artificiale](https://docs.microsoft.com/it-it/azure/cognitive-services/computer-vision/concept-recognizing-text)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}