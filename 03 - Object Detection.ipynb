{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Rilevamento degli oggetti\r\n",
        "\r\n",
        "Il *rilevamento degli oggetti* è una forma di visione artificiale in cui viene eseguito il training di un modello di apprendimento automatico per classificare le singole istanze di oggetti in un'immagine e indicare un *riquadro delimitatore del testo* che contrassegna la sua posizione. Puoi considerarlo come una progressione dalla *classificazione delle immagini* (in cui il modello risponde alla domanda \"cosa riguarda questa immagine?\") allo sviluppo di soluzioni in cui possiamo chiedere al modello \"che oggetti ci sono in questa immagine, e dove sono?\".\r\n",
        "\r\n",
        "![Un robot che identifica la frutta](./images/object-detection.jpg)\r\n",
        "\r\n",
        "Ad esempio, un negozio di alimentari potrebbe utilizzare un modello di rilevamento degli oggetti per implementare un sistema di cassa automatizzato che scansiona un nastro trasportatore utilizzando una telecamera ed è in grado di identificare articoli specifici senza la necessità di posizionare ogni articolo sul nastro e scansionarlo individualmente.\r\n",
        "\r\n",
        "Il servizio cognitivo **Visione personalizzata** in Microsoft Azure fornisce una soluzione basata su cloud per la creazione e la pubblicazione di modelli di rilevamento di oggetti personalizzati.\r\n",
        "\r\n",
        "## Crea una risorsa Visione personalizzata\r\n",
        "\r\n",
        "Per utilizzare il servizio Visione personalizzata, hai bisogno di una risorsa di Azure che puoi utilizzare per eseguire il training di un modello e una risorsa con la quale puoi pubblicarlo per usare le applicazioni. Puoi usare la stessa risorsa per ognuna di queste attività, o puoi usare risorse diverse per ognuna per allocare i costi separatamente, a condizione che entrambe le risorse siano create nella stessa area geografica. La risorsa per una (o entrambe) le attività può essere una risorsa generale di **Servizi cognitivi**, o una risorsa specifica di **Visione personalizzata**. Usa le istruzioni seguenti per creare una nuova risorsa **Visione personalizzata** (o puoi usare una risorsa esistente se ne hai una).\r\n",
        "\r\n",
        "1. In una nuova scheda del browser, apri il portale Azure all'indirizzo [https://portal.azure.com](https://portal.azure.com) e accedi utilizzando l'account Microsoft associato alla tua sottoscrizione di Azure.\r\n",
        "2. Seleziona il pulsante **&#65291;Crea una risorsa**, cerca *visione personalizzata* e crea una risorsa di **Visione personalizzata** con le impostazioni seguenti:\r\n",
        "    - **Crea opzioni**: Entrambe\r\n",
        "    - **Sottoscrizione**: *la tua sottoscrizione di Azure*\r\n",
        "    - **Gruppo di risorse**: *Seleziona o crea un gruppo di risorse con un nome univoco*\r\n",
        "    - **Nome**: *Immetti un nome univoco*\r\n",
        "    - **Località training**: *Scegli una qualsiasi area disponibile*\r\n",
        "    - **Piano tariffario training**: F0\r\n",
        "    - **Località previsione**: *Lo stesso del percorso di training*\r\n",
        "    - **Piano tariffario previsione**: F0\r\n",
        "\r\n",
        "    > **Nota**: Se disponi già di un servizio di visione personalizzata F0 nella tua sottoscrizione, seleziona **S0**.\r\n",
        "\r\n",
        "3. Attendi che la risorsa venga creata.\r\n",
        "\r\n",
        "## Crea un progetto Visione personalizzata\r\n",
        "\r\n",
        "Per eseguire il training di un modello di rilevamento degli oggetti, è necessario creare un progetto Visione personalizzata basato sulla risorsa di training. Per farlo, userai il portale Visione personalizzata.\r\n",
        "\r\n",
        "1. In una nuova scheda del browser, apri il portale Visione personalizzata all'indirizzo [ https://customvision.ai](https://customvision.ai) e accedi utilizzando l'account Microsoft associato alla tua sottoscrizione di Azure.\r\n",
        "2. Crea un nuovo progetto con le impostazioni seguenti:\r\n",
        "    - **Nome**: Rilevamento alimentari\r\n",
        "    - **Descrizione**: Rilevamento di oggetti per negozi di alimentari.\r\n",
        "    - **Risorsa**: *La risorsa Visione personalizzata creata in precedenza*\r\n",
        "    - **Tipi progetto**: Rilevamento degli oggetti\r\n",
        "    - **Domini**: Generale\r\n",
        "3. Attendi che il progetto venga creato e aperto nel browser.\r\n",
        "\r\n",
        "## Aggiungi e aggiungi tag alle immagini\r\n",
        "\r\n",
        "Per eseguire il training di un modello di rilevamento degli oggetti, è necessario caricare le immagini che contengono le classi che vuoi fare identificare dal modello e aggiungervi tag per indicare i riquadri delimitatori del testo per ogni istanza dell'oggetto.\r\n",
        "\r\n",
        "1. Scaricare ed estrarre le immagini di training da https://aka.ms/fruit-objects. La cartella estratta contiene una raccolta di immagini di frutta. **Nota:** come soluzione temporanea, nel caso non sia possibile accedere alle immagini di training, passare a https://www.github.com e quindi a https://aka.ms/fruit-objects. \r\n",
        "2. Nel portale Visione personalizzata [https://customvision.ai](https://customvision.ai), verifica di trovarti nel tuo progetto di rilevamento oggetti _Grocery Detection_. Quindi seleziona **Aggiungi immagini** e carica tutte le immagini nella cartella estratta.\r\n",
        "\r\n",
        "![Carica le immagini scaricate facendo clic su aggiungi immagini.](./images/fruit-upload.jpg)\r\n",
        "\r\n",
        "3. Una volta che le immagini sono state caricate, seleziona la prima per aprirla.\r\n",
        "4. Tieni il mouse sopra qualsiasi oggetto nell'immagine fino a quando viene visualizzata un'area geografica rilevata automaticamente come l'immagine seguente. Seleziona quindi l'oggetto e, se necessario, ridimensiona l'area geografica per circondarlo.\r\n",
        "\r\n",
        "![L'area geografica predefinita di un oggetto](./images/object-region.jpg)\r\n",
        "\r\n",
        "In alternativa, puoi semplicemente trascinare il mouse intorno all'oggetto per creare un'area geografica.\r\n",
        "\r\n",
        "5. Quando l'area geografica circonda l'oggetto, aggiungi un nuovo tag con il tipo di oggetto appropriato (*mela*, *banana* o *arancia*) come mostrato qui:\r\n",
        "\r\n",
        "![Un oggetto con tag in un'immagine](./images/object-tag.jpg)\r\n",
        "\r\n",
        "6. Seleziona e aggiungi tag a ogni altro oggetto nell'immagine, ridimensionando le aree geografiche e aggiungendo nuovi tag come necessario.\r\n",
        "\r\n",
        "![Due oggetti con tag in un'immagine](./images/object-tags.jpg)\r\n",
        "\r\n",
        "7. Usa il link **>** sulla destra per passare all'immagine successiva e aggiungere un tag ai suoi oggetti. Poi continua a lavorare su tutta la raccolta di immagini, aggiungendo un tag a ogni mela, banana e arancia.\r\n",
        "\r\n",
        "8. Quando hai finito di aggiungere tag all'ultima immagine, chiudi l'editor **Dettaglio immagine** e nella pagina **Immagini training**, in **Tag**, seleziona **Con tag** per vedere tutte le tue immagini con tag:\r\n",
        "\r\n",
        "![Immagini con tag in un progetto](./images/tagged-images.jpg)\r\n",
        "\r\n",
        "## Esegui il training e il test di un modello\r\n",
        "\r\n",
        "Ora che hai aggiunto un tag alle immagini nel tuo progetto, è tutto pronto per eseguire il training di un modello.\r\n",
        "\r\n",
        "1. Nel progetto Visione personalizzata, fai clic su **Esegui il training** per eseguire il training di un modello di rilevamento degli oggetti utilizzando le immagini con tag. Seleziona l'opzione **Training rapido**.\r\n",
        "2. Attendi il completamento del training (potrebbe richiedere circa dieci minuti), quindi esamina le metriche di prestazione *Precisione*, *Richiamo*, e *AP*, che misurano l'accuratezza della previsione del modello di classificazione e dovrebbero essere tutte elevate.\r\n",
        "3. In alto a destra della pagina, fai clic su **Test rapido** e poi nella casella **URL immagine**, inserisci `https://aka.ms/apple-orange` e visualizza la previsione che viene generata. Chiudi quindi la finestra **Test rapido**.\r\n",
        "\r\n",
        "## Pubblica e consuma il modello di rilevamento degli oggetti\r\n",
        "\r\n",
        "Ora è tutto pronto per pubblicare il tuo modello sottoposto a training e usarlo da un'applicazione client.\r\n",
        "\r\n",
        "1. Nella parte in alto a sinistra della pagina **Prestazioni**, fai clic su **&#128504; Pubblica** per pubblicare il modello sottoposto a training con le seguenti impostazioni:\r\n",
        "    - **Nome modello**: detect-produce\r\n",
        "    - **Risorsa di previsione**: *La tua risorsa di **previsione** personalizzata*.\r\n",
        "\r\n",
        "### (!) Verifica \r\n",
        "Hai usato lo stesso nome del modello: **detect-produce**? \r\n",
        "\r\n",
        "2. Dopo la pubblicazione, fai clic sull'icona delle *impostazioni * (&#9881;) in alto a destra della pagina **Prestazioni** per visualizzare le impostazioni del progetto. Quindi, in **Generali** (a sinistra), copia l'**ID progetto**. Scorri verso il basso e incollalo nella cella del codice sotto il passaggio 5 sostituendo **YOUR_PROJECT_ID**. \r\n",
        "\r\n",
        "> (*se hai usato una risorsa di **Servizi cognitivi** anziché creare una risorsa di **Visione personalizzata** all'inizio di questo esercizio, puoi copiare la sua chiave e l'endpoint dal lato destro delle impostazioni del progetto, incollarlo nella cella di codice di seguito, ed eseguirlo per vedere i risultati. Altrimenti, continua a completare i passaggi seguenti per ottenere la chiave e l'endpoint per la tua risorsa di previsione Visione personalizzata*).\r\n",
        "\r\n",
        "3. In alto a sinistra della pagina **Impostazioni progetto**, fai clic sull'icona *Galleria progetti* (&#128065;) per tornare alla pagina iniziale del portale Visione personalizzata, dove adesso è elencato il tuo progetto.\r\n",
        "\r\n",
        "4. Nella pagina iniziale del portale Visione personalizzata, in alto a destra, fai clic sull'icona delle *impostazioni* (&#9881;) per visualizzare le impostazioni del tuo servizio Visione personalizzata. Quindi, in **Risorse**, espandi la tua risorsa di *previsione* (<u>non</u> la risorsa di training) e copia i suoi valori **Chiave** ed **Endpoint** nella cella di codice sotto il passaggio 5, sostituendo **YOUR_KEY** e **YOUR_ENDPOINT**.\r\n",
        "\r\n",
        "### (!) Verifica \r\n",
        "Se stai usando una risorsa **Visione personalizzata**, hai usato la risorsa di **previsione** (<u>non</u> la risorsa di training)?\r\n",
        "\r\n",
        "5. Esegui la cella di codice di seguito facendo clic sul pulsante Esegui cella <span>&#9655;</span> (in alto a sinistra della cella) per impostare le variabili sui valori dell'ID del progetto, della chiave e dell'endpoint."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "project_id = 'YOUR_PROJECT_ID' # Replace with your project ID\r\n",
        "cv_key = 'YOUR_KEY' # Replace with your prediction resource primary key\r\n",
        "cv_endpoint = 'YOUR_ENDPOINT' # Replace with your prediction resource endpoint\r\n",
        "\r\n",
        "model_name = 'detect-produce' # this must match the model name you set when publishing your model iteration exactly (including case)!\r\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692485387
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora puoi utilizzare la chiave e l'endpoint con un client di Visione personalizzata per connetterti al tuo modello di rilevamento degli oggetti.\r\n",
        "\r\n",
        "Esegui la seguente cella di codice, che usa il tuo modello per rilevare i singoli articoli di produzione in un'immagine.\r\n",
        "\r\n",
        "> **Nota**: Non preoccuparti troppo dei dettagli del codice. Utilizza l'SDK Python per il servizio Visione personalizzata per inviare un'immagine al tuo modello e recuperare le previsioni per gli oggetti rilevati. Ogni previsione consiste in un nome di classe (*mela*, *banana* o *arancia*) e in coordinate del *riquadro delimitatore del testo* che indicano il punto dell'immagine in cui l'oggetto previsto è stato rilevato. Il codice usa quindi queste informazioni per disegnare un riquadro etichettato intorno a ciascun oggetto sull'immagine."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\r\n",
        "from msrest.authentication import ApiKeyCredentials\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from PIL import Image, ImageDraw, ImageFont\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Load a test image and get its dimensions\r\n",
        "test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\r\n",
        "test_img = Image.open(test_img_file)\r\n",
        "test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\r\n",
        "\r\n",
        "# Get a prediction client for the object detection model\r\n",
        "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\r\n",
        "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\r\n",
        "\r\n",
        "print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\r\n",
        "\r\n",
        "# Detect objects in the test image\r\n",
        "with open(test_img_file, mode=\"rb\") as test_data:\r\n",
        "    results = predictor.detect_image(project_id, model_name, test_data)\r\n",
        "\r\n",
        "# Create a figure to display the results\r\n",
        "fig = plt.figure(figsize=(8, 8))\r\n",
        "plt.axis('off')\r\n",
        "\r\n",
        "# Display the image with boxes around each detected object\r\n",
        "draw = ImageDraw.Draw(test_img)\r\n",
        "lineWidth = int(np.array(test_img).shape[1]/100)\r\n",
        "object_colors = {\r\n",
        "    \"apple\": \"lightgreen\",\r\n",
        "    \"banana\": \"yellow\",\r\n",
        "    \"orange\": \"orange\"\r\n",
        "}\r\n",
        "for prediction in results.predictions:\r\n",
        "    color = 'white' # default for 'other' object tags\r\n",
        "    if (prediction.probability*100) > 50:\r\n",
        "        if prediction.tag_name in object_colors:\r\n",
        "            color = object_colors[prediction.tag_name]\r\n",
        "        left = prediction.bounding_box.left * test_img_w \r\n",
        "        top = prediction.bounding_box.top * test_img_h \r\n",
        "        height = prediction.bounding_box.height * test_img_h\r\n",
        "        width =  prediction.bounding_box.width * test_img_w\r\n",
        "        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\r\n",
        "        draw.line(points, fill=color, width=lineWidth)\r\n",
        "        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\r\n",
        "plt.imshow(test_img)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692585672
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizza le previsioni risultanti, che mostrano gli oggetti rilevati e la probabilità per ogni previsione."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}