{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analisi delle immagini con il servizio Visione artificiale\r\n",
        "\r\n",
        "![Un robot che tiene in mano un'immagine](./images/computer_vision.jpg)\r\n",
        "\r\n",
        "La *visione artificiale* è una branca dell'intelligenza artificiale (IA) che esplora lo sviluppo di sistemi IA che possono \"vedere\" il mondo, sia in tempo reale attraverso una telecamera o analizzando immagini e video. Questo è reso possibile dal fatto che le immagini digitali sono essenzialmente solo matrici di valori numerici di pixel, e possiamo usare questi valori di pixel come *funzionalità* per eseguire il training di modelli di machine learning che possono classificare le immagini, rilevare oggetti discreti in un'immagine e anche generare riassunti testuali di fotografie.\r\n",
        "\r\n",
        "## Usa i servizi cognitivi della visione artificiale\r\n",
        "\r\n",
        "Microsoft Azure include una serie di *servizi cognitivi* che incapsulano funzioni IA comuni, compresi alcuni che possono aiutarti a sviluppare soluzioni di visione artificiale.\r\n",
        "\r\n",
        "Il servizio cognitivo *Visione artificiale* fornisce un ovvio punto di partenza per la nostra esplorazione della visione artificiale in Azure. Utilizza modelli di apprendimento automatico sottoposti in precedenza a training per analizzare le immagini ed estrarre informazioni su di esse.\r\n",
        "\r\n",
        "Ad esempio, supponiamo che Northwind Traders abbia deciso di implementare un \"negozio intelligente\", in cui i servizi di IA monitorano il negozio per identificare i clienti che richiedono assistenza e guidare i dipendenti che devono aiutarli. Utilizzando il servizio di Visione artificiale, le immagini riprese dalle telecamere in tutto il negozio possono essere analizzate per fornire descrizioni significative di ciò che rappresentano.\r\n",
        "\r\n",
        "### Crea una risorsa di servizi cognitivi\r\n",
        "\r\n",
        "Iniziamo creando una risorsa di **Servizi cognitivi** nella tua sottoscrizione di Azure:\r\n",
        "\r\n",
        "1. In un'altra scheda del browser, apri il portale di Azure all'indirizzo https://portal.azure.com, accedendo con il tuo account Microsoft.\r\n",
        "2. Fai clic sul pulsante **&#65291;Crea una risorsa**, cerca *Servizi cognitivi* e crea una risorsa di **Servizi cognitivi** con le impostazioni seguenti:\r\n",
        "    - **Sottoscrizione**: *La tua sottoscrizione di Azure*.\r\n",
        "    - **Gruppo di risorse**: *Seleziona o crea un gruppo di risorse con un nome univoco*.\r\n",
        "    - **Area geografica**: *Scegli una qualsiasi area disponibile*:\r\n",
        "    - **Nome**: *Immetti un nome univoco*.\r\n",
        "    - **Piano tariffario**: S0\r\n",
        "    - **Confermo di aver letto e compreso gli avvisi**: Selezionato.\r\n",
        "3. Attendi il completamento della distribuzione. Vai quindi alla tua risorsa di servizi cognitivi e, nella pagina **Panoramica**, fai clic sul link per gestire le chiavi per il servizio. Avrai bisogno dell'endpoint e delle chiavi per connetterti alla tua risorsa di servizi cognitivi dalle applicazioni client.\r\n",
        "\r\n",
        "### Ottieni la chiave e l'endpoint per la tua risorsa di Servizi cognitivi\r\n",
        "\r\n",
        "Per usare la risorsa di servizi cognitivi, le applicazioni client hanno bisogno del loro endpoint e della chiave di autenticazione:\r\n",
        "\r\n",
        "1. Nel portale di Azure, nella pagina **Chiavi ed endpoint** per la tua risorsa di servizio cognitivo, copia la **Key1** per la tua risorsa e incollala nel codice sottostante, sostituendo **YOUR_COG_KEY**.\r\n",
        "2. Copia l'**endpoint** per la tua risorsa e incollalo nel codice sottostante, sostituendo **YOUR_COG_ENDPOINT**.\r\n",
        "3. Esegui il codice seguente selezionando la cella e poi facendo clic sul pulsante **Esegui cella** (&#9655;) a sinistra della cella."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599691487445
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora che hai impostato la chiave e l'endpoint, puoi usare il servizio Visione artificiale per analizzare un'immagine.\r\n",
        "\r\n",
        "Esegui la cella seguente per ottenere una descrizione di un'immagine nel file */data/vision/store_cam1.jpg*."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from python_code import vision\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get the path to an image file\r\n",
        "image_path = os.path.join('data', 'vision', 'store_cam1.jpg')\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Get a description from the computer vision service\r\n",
        "image_stream = open(image_path, \"rb\")\n",
        "description = computervision_client.describe_image_in_stream(image_stream)\n",
        "\n",
        "# Display image and caption (code in helper_scripts/vision.py)\r\n",
        "vision.show_image_caption(image_path, description)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599691518279
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sembra sufficientemente accurato.\r\n",
        "\r\n",
        "Proviamo un'altra immagine."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the path to an image file\r\n",
        "image_path = os.path.join('data', 'vision', 'store_cam2.jpg')\n",
        "\n",
        "# Get a description from the computer vision service\r\n",
        "image_stream = open(image_path, \"rb\")\n",
        "description = computervision_client.describe_image_in_stream(image_stream)\n",
        "\n",
        "# Display image and caption (code in helper_scripts/vision.py)\r\n",
        "vision.show_image_caption(image_path, description)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599691524330
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Di nuovo, la didascalia suggerita sembra essere abbastanza accurata.\r\n",
        "\r\n",
        "## Analizza le caratteristiche dell'immagine\r\n",
        "\r\n",
        "Finora hai usato il servizio Visione artificiale per generare una didascalia descrittiva per un paio di immagini, ma puoi fare molto di più. Il servizio Visione artificiale fornisce funzionalità di analisi che possono estrarre informazioni dettagliate come:\r\n",
        "\r\n",
        "- Le posizioni dei tipi comuni di oggetti rilevati nell'immagine.\r\n",
        "- Posizione ed età approssimativa dei volti umani nell'immagine.\r\n",
        "- Se l'immagine contiene contenuti \"da adulti\", \"spinti\" o \"cruenti\".\r\n",
        "- Tag pertinenti che potrebbero essere associati all'immagine in un database per renderla facile da trovare.\r\n",
        "\r\n",
        "Esegui il codice seguente per analizzare l'immagine di un acquirente."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the path to an image file\r\n",
        "image_path = os.path.join('data', 'vision', 'store_cam1.jpg')\n",
        "\n",
        "# Specify the features we want to analyze\r\n",
        "features = ['Description', 'Tags', 'Adult', 'Objects', 'Faces']\n",
        "\n",
        "# Get an analysis from the computer vision service\r\n",
        "image_stream = open(image_path, \"rb\")\n",
        "analysis = computervision_client.analyze_image_in_stream(image_stream, visual_features=features)\n",
        "\n",
        "# Show the results of analysis (code in helper_scripts/vision.py)\r\n",
        "vision.show_image_analysis(image_path, analysis)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599691530747
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scopri di più\r\n",
        "\r\n",
        "Oltre alle funzionalità che hai esplorato in questo notebook, il servizio cognitivo Visione artificiale include la possibilità di:\r\n",
        "\r\n",
        "- Identificare personaggi famosi nelle immagini.\r\n",
        "- Rilevare i loghi dei marchi in un'immagine.\r\n",
        "- Eseguire il riconoscimento ottico dei caratteri (OCR) per leggere il testo in un'immagine.\r\n",
        "\r\n",
        "Per saperne di più sul servizio cognitivo Visione artificiale, consulta la [documentazione di Visione artificiale](https://docs.microsoft.com/azure/cognitive-services/computer-vision/)\r\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}